{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration: {'Audit_JDBC': {'AUDIT_JDBC_connection_userPassword_password': 'Sa*201299', 'AUDIT_JDBC_drivers': 'C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar', 'AUDIT_JDBC_connection_driverClass': 'com.mysql.cj.jdbc.Driver', 'AUDIT_JDBC_mappingFile': 'mysql_id', 'AUDIT_JDBC_connection_userPassword_userId': 'root', 'AUDIT_JDBC_connection_jdbcUrl': 'jdbc:mysql://localhost:3306/sqops_dataraise'}, 'queries': {'LOCAL_TO_DBBRUT_QUERY': 'select distinct PROJECT_NAME, JOB_NAME, JOB_PATH, JOB_VERSION, Talend_Version from audit_jobs_delta where talend_version is not null and niveau is not null', 'LOCAL_TO_DBBRUT_FILTER_name_elementNode': \"where aud_nameElementNode in ('DBNAME','TYPE','QUERY','TABLE','FILENAME','TEMPDIR','sql','query')\", 'LOCAL_TO_DBBRUT_FILTER_columnName_metadata': \"where aud_columnName not in ('errorCode','errorMessage')\", 'LOCAL_TO_DBBRUT_QUERY_ROUTINES': 'select distinct PROJECT_NAME, ROUTINE_NAME, ROUTINE_PATH, ROUTINE_VERSION, Talend_Version from audit_routines where talend_version is not null', 'LOCAL_TO_DBBRUT_QUERY_CONTEXTGROUP': 'select distinct PROJECT_NAME, CONTEXT_NAME, CONTEXT_PATH, CONTEXT_VERSION, Talend_Version from audit_contextgroup where talend_version is not null', 'LOCAL_TO_DBBRUT_QUERY_METADATA': 'select distinct PROJECT_NAME, METADATA_NAME, METADATA_PATH, METADATA_VERSION, Talend_Version from audit_metadata where talend_version is not null', 'TRANSVERSE_QUERY_LASTEXECUTIONDATE': 'SELECT MAX(lastexecutiondate) as lastexecutiondate FROM executiondate', 'LOCAL_TO_DBBRUT_QUERY_JOBLETS': 'select distinct PROJECT_NAME, JOBLET_NAME, JOBLET_PATH, JOBLET_VERSION, Talend_Version from audit_joblets where talend_version is not null', 'aud_elementnode': 'select distinct namejob, nameproject from aud_elementnode where NameJob not in (select job_name from audit_jobs)', 'aud_contextjob': 'select distinct namejob, nameproject from aud_contextjob where NameJob not in (select job_name from audit_jobs)', 'aud_node': 'select distinct namejob, nameproject from aud_node where NameJob not in (select job_name from audit_jobs)', 'NodeJoinElementnode': \"select distinct N.NameProject, N.NameJob, N.aud_componentValue from aud_node N inner join aud_elementnode E on (N.namejob = E.namejob and N.nameproject = E.nameproject and N.aud_componentValue = E.aud_ComponementValue) where E.aud_nameElementNode = 'ACTIVATE' and E.aud_valueElementNode = 'false' and E.exec_date = (select max(lastexecutiondate) from executiondate)\", 'aud_bigdata': 'select distinct namejob, nameproject from aud_bigdata where NameJob not in (select job_name from audit_jobs)', 'aud_metadata': 'select distinct namejob, nameproject from aud_metadata where NameJob not in (select job_name from audit_jobs)', 'MetadataJoinElemntnode': \"select  distinct N.NameProject, N.NameJob, N.aud_componentValue from aud_metadata N inner join aud_elementnode E on (N.namejob = E.namejob and N.nameproject = E.nameproject and N.aud_componentValue = E.aud_ComponementValue) where E.aud_nameElementNode = 'ACTIVATE' and E.aud_valueElementNode = 'false' and E.exec_date = (select max(lastexecutiondate) from executiondate)\"}, 'insert_queries': {'aud_agg_aggregate': \"INSERT IGNORE INTO aud_agg_aggregate (NameProject, namejob, aud_componentValue, aud_valueElementRef_input, aud_valueElementRef_output, aud_valueElementRef_function) SELECT a.NameProject, a.namejob, a.aud_componentValue, a.aud_valueElementRef as aud_valueElementRef_input, b.aud_valueElementRef as aud_valueElementRef_output, 'GROUPBY' as aud_valueElementRef_function FROM aud_elementvaluenode a INNER JOIN aud_elementvaluenode b ON a.aud_id : b.aud_id WHERE a.aud_typeField IN ('GROUPBYS') AND b.aud_typeField IN ('GROUPBYS') AND a.aud_elementRef : 'INPUT_COLUMN' AND b.aud_elementRef : 'OUTPUT_COLUMN' AND a.aud_componentName LIKE 'tAggregate%' AND b.aud_componentName LIKE 'tAggregate%' UNION ALL SELECT a.NameProject, a.namejob, a.aud_componentValue, a.aud_valueElementRef as aud_valueElementRef_input, b.aud_valueElementRef as aud_valueElementRef_output , c.aud_valueElementRef as aud_valueElementRef_function FROM aud_elementvaluenode a INNER JOIN aud_elementvaluenode b ON a.aud_id : b.aud_id INNER JOIN aud_elementvaluenode c ON a.aud_id : c.aud_id WHERE a.aud_componentName LIKE 'tAggregate%' AND a.aud_typeField : 'OPERATIONS' AND b.aud_componentName LIKE 'tAggregate%' AND b.aud_typeField : 'OPERATIONS' AND c.aud_componentName LIKE 'tAggregate%' AND c.aud_typeField : 'OPERATIONS' AND a.aud_elementRef : 'INPUT_COLUMN' AND b.aud_elementRef : 'OUTPUT_COLUMN' AND c.aud_elementRef : 'FUNCTION'\", 'aud_elementnode': 'INSERT IGNORE INTO aud_elementnode (aud_componentName,aud_field,aud_nameElementNode,aud_show,aud_valueElementNode,aud_ComponementValue,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)', 'aud_contextjob': 'INSERT IGNORE INTO aud_contextjob (aud_environementContext,aud_nameContext,aud_prompt,aud_promptNeeded,aud_typeContext,aud_valueContext,aud_repositoryContextId,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?,?)', 'aud_contextgroupdetail': 'INSERT IGNORE INTO aud_contextgroupdetail (aud_nameContext,aud_commentContext,NameProject,NameContextGroup,exec_date) VALUES (?, ?, ?, ?, ?)', 'aud_node': 'INSERT IGNORE INTO aud_node (aud_componentName , aud_componentVersion , aud_offsetLabelX , aud_offsetLabelY , aud_posX , aud_posY , aud_componentValue , NameProject , NameJob , exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?,?)', 'aud_bigdata': 'INSERT IGNORE INTO aud_bigdata (aud_field,aud_name,aud_show,aud_value,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?)', 'aud_bigdata_elementvalue': 'INSERT IGNORE INTO aud_bigdata_elementvalue (aud_elementRef,aud_value,aud_name,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?)', 'aud_metadata': 'INSERT INTO aud_metadata(aud_connector,aud_labelConnector,aud_nameComponentView,aud_comment,aud_key,aud_length,aud_columnName,aud_nullable,aud_pattern,aud_precision,aud_sourceType,aud_type,aud_usefulColumn,aud_originalLength,aud_defaultValue,aud_componentValue,aud_componentName,NameProject,NameJob,exec_date) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?) ON DUPLICATE KEY UPDATE aud_connector = VALUES(aud_connector), aud_labelConnector = VALUES(aud_labelConnector), aud_nameComponentView = VALUES(aud_nameComponentView), aud_comment = VALUES(aud_comment), aud_key = VALUES(aud_key), aud_length = VALUES(aud_length), aud_columnName = VALUES(aud_columnName), aud_nullable = VALUES(aud_nullable), aud_pattern = VALUES(aud_pattern), aud_precision = VALUES(aud_precision), aud_sourceType = VALUES(aud_sourceType), aud_type = VALUES(aud_type), aud_usefulColumn = VALUES(aud_usefulColumn), aud_originalLength = VALUES(aud_originalLength), aud_defaultValue = VALUES(aud_defaultValue), aud_componentValue = VALUES(aud_componentValue), aud_componentName = VALUES(aud_componentName), exec_date = VALUES(exec_date)'}, 'Directories': {'items_directory': 'C:/Users/sonia/Downloads/KEOLISTOURS/KEOLISTOURS/process'}, 'database': {'type': 'mysql', 'postgresql': {'user': 'postgres', 'password': 'your_password', 'host': 'localhost', 'port': 5432, 'dbname': 'dataraise_test'}, 'mysql': {'user': 'root', 'password': 'Sa*201299', 'host': '127.0.0.1', 'port': 3306, 'dbname': 'sqops_dataraise'}, 'sqlite': {'filepath': 'path/to/your/sqlite.db'}}, 'ORA_ECH_ADMECH': {'CTX_ORA_ECH_ADMECH_Server': 'echdfr.cfo1dl4btaz7.eu-west-1.rds.amazonaws.com', 'CTX_ORA_ECH_ADMECH_Password': '', 'CTX_ORA_ECH_ADMECH_Login': 'ADM_ECH', 'CTX_ORA_ECH_ADMECH_Port': '1521', 'CTX_ORA_ECH_ADMECH_AdditionalParams': '', 'CTX_ORA_ECH_ADMECH_Schema': 'ADM_ECH', 'CTX_ORA_ECH_ADMECH_Sid': 'echdfr'}}\n",
      "JDBC Parameters: {'AUDIT_JDBC_connection_userPassword_password': 'Sa*201299', 'AUDIT_JDBC_drivers': 'C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar', 'AUDIT_JDBC_connection_driverClass': 'com.mysql.cj.jdbc.Driver', 'AUDIT_JDBC_mappingFile': 'mysql_id', 'AUDIT_JDBC_connection_userPassword_userId': 'root', 'AUDIT_JDBC_connection_jdbcUrl': 'jdbc:mysql://localhost:3306/sqops_dataraise'}\n",
      "JDBC Driver: com.mysql.cj.jdbc.Driver\n",
      "JDBC URL: jdbc:mysql://localhost:3306/sqops_dataraise\n",
      "JDBC User: root\n",
      "JDBC JAR: C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar\n",
      "parameter: items_directory -> C:/Users/sonia/Downloads/KEOLISTOURS/KEOLISTOURS/process\n",
      "parameter: TRANSVERSE_QUERY_LASTEXECUTIONDATE -> SELECT MAX(lastexecutiondate) as lastexecutiondate FROM executiondate\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Add the directory containing AUD_304_ALIMMETADATA.py to the Python path\n",
    "sys.path.append(os.path.dirname('C:/Users/sonia/Desktop/SQOPS_SERVER_PY/SQOPS_SERVER_PY/Local_to_brut/AUD_304_ALIMMETADATA.py'))\n",
    "from AUD_304_ALIMMETADATA import AUD_304_ALIMMETADATA\n",
    "from config import Config  # Assuming Config class is defined in config.py\n",
    "from XML_parse import XMLParser  # Importing the XMLParser class\n",
    "from database import Database  # Assuming Database class is defined in database.py\n",
    "\n",
    "def main():\n",
    "    db = None\n",
    "    config_file = \"configs/config.yaml\"\n",
    "    config = Config(config_file)\n",
    "\n",
    "    # Retrieve JDBC parameters and create a Database instance\n",
    "    jdbc_params = config.get_jdbc_parameters()\n",
    "    logging.debug(f\"JDBC Parameters: {jdbc_params}\")\n",
    "\n",
    "    db = Database(jdbc_params)\n",
    "    db.set_jdbc_parameters(jdbc_params)  # Set JDBC parameters if needed\n",
    "    db.connect_JDBC()  # Test the JDBC connection\n",
    "\n",
    "    items_directory = config.get_param('Directories', 'items_directory')\n",
    "    xml_parser = XMLParser()\n",
    "    parsed_files_data = xml_parser.loop_parse(items_directory)\n",
    "    logging.debug(f\"Parsed Files Data: {parsed_files_data}\")\n",
    "\n",
    "    # Call the function with the configuration and parsed data\n",
    "    AUD_304_ALIMMETADATA(config, db, parsed_files_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration: {'Audit_JDBC': {'AUDIT_JDBC_connection_userPassword_password': 'Sa*201299', 'AUDIT_JDBC_drivers': 'C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar', 'AUDIT_JDBC_connection_driverClass': 'com.mysql.cj.jdbc.Driver', 'AUDIT_JDBC_mappingFile': 'mysql_id', 'AUDIT_JDBC_connection_userPassword_userId': 'root', 'AUDIT_JDBC_connection_jdbcUrl': 'jdbc:mysql://localhost:3306/sqops_dataraise?allowLoadLocalInfile=true'}, 'queries': {'LOCAL_TO_DBBRUT_QUERY': 'select distinct PROJECT_NAME, JOB_NAME, JOB_PATH, JOB_VERSION, Talend_Version from audit_jobs_delta where talend_version is not null and niveau is not null', 'LOCAL_TO_DBBRUT_FILTER_name_elementNode': \"where aud_nameElementNode in ('DBNAME','TYPE','QUERY','TABLE','FILENAME','TEMPDIR','sql','query')\", 'LOCAL_TO_DBBRUT_FILTER_columnName_metadata': \"where aud_columnName not in ('errorCode','errorMessage')\", 'LOCAL_TO_DBBRUT_QUERY_ROUTINES': 'select distinct PROJECT_NAME, ROUTINE_NAME, ROUTINE_PATH, ROUTINE_VERSION, Talend_Version from audit_routines where talend_version is not null', 'LOCAL_TO_DBBRUT_QUERY_CONTEXTGROUP': 'select distinct PROJECT_NAME, CONTEXT_NAME, CONTEXT_PATH, CONTEXT_VERSION, Talend_Version from audit_contextgroup where talend_version is not null', 'LOCAL_TO_DBBRUT_QUERY_METADATA': 'select distinct PROJECT_NAME, METADATA_NAME, METADATA_PATH, METADATA_VERSION, Talend_Version from audit_metadata where talend_version is not null', 'TRANSVERSE_QUERY_LASTEXECUTIONDATE': 'SELECT MAX(lastexecutiondate) as lastexecutiondate FROM executiondate', 'LOCAL_TO_DBBRUT_QUERY_JOBLETS': 'select distinct PROJECT_NAME, JOBLET_NAME, JOBLET_PATH, JOBLET_VERSION, Talend_Version from audit_joblets where talend_version is not null', 'aud_elementnode': 'select distinct namejob, nameproject from aud_elementnode where NameJob not in (select job_name from audit_jobs)', 'aud_contextjob': 'select distinct namejob, nameproject from aud_contextjob where NameJob not in (select job_name from audit_jobs)', 'aud_node': 'select distinct namejob, nameproject from aud_node where NameJob not in (select job_name from audit_jobs)', 'NodeJoinElementnode': \"select distinct N.NameProject, N.NameJob, N.aud_componentValue from aud_node N inner join aud_elementnode E on (N.namejob = E.namejob and N.nameproject = E.nameproject and N.aud_componentValue = E.aud_ComponementValue) where E.aud_nameElementNode = 'ACTIVATE' and E.aud_valueElementNode = 'false' and E.exec_date = (select max(lastexecutiondate) from executiondate)\", 'aud_bigdata': 'select distinct namejob, nameproject from aud_bigdata where NameJob not in (select job_name from audit_jobs)', 'aud_metadata': 'select distinct namejob, nameproject from aud_metadata where NameJob not in (select job_name from audit_jobs)', 'MetadataJoinElemntnode': \"select  distinct N.NameProject, N.NameJob, N.aud_componentValue from aud_metadata N inner join aud_elementnode E on (N.namejob = E.namejob and N.nameproject = E.nameproject and N.aud_componentValue = E.aud_ComponementValue) where E.aud_nameElementNode = 'ACTIVATE' and E.aud_valueElementNode = 'false' and E.exec_date = (select max(lastexecutiondate) from executiondate)\"}, 'insert_queries': {'aud_agg_aggregate': \"INSERT IGNORE INTO aud_agg_aggregate (NameProject, namejob, aud_componentValue, aud_valueElementRef_input, aud_valueElementRef_output, aud_valueElementRef_function) SELECT a.NameProject, a.namejob, a.aud_componentValue, a.aud_valueElementRef as aud_valueElementRef_input, b.aud_valueElementRef as aud_valueElementRef_output, 'GROUPBY' as aud_valueElementRef_function FROM aud_elementvaluenode a INNER JOIN aud_elementvaluenode b ON a.aud_id : b.aud_id WHERE a.aud_typeField IN ('GROUPBYS') AND b.aud_typeField IN ('GROUPBYS') AND a.aud_elementRef : 'INPUT_COLUMN' AND b.aud_elementRef : 'OUTPUT_COLUMN' AND a.aud_componentName LIKE 'tAggregate%' AND b.aud_componentName LIKE 'tAggregate%' UNION ALL SELECT a.NameProject, a.namejob, a.aud_componentValue, a.aud_valueElementRef as aud_valueElementRef_input, b.aud_valueElementRef as aud_valueElementRef_output , c.aud_valueElementRef as aud_valueElementRef_function FROM aud_elementvaluenode a INNER JOIN aud_elementvaluenode b ON a.aud_id : b.aud_id INNER JOIN aud_elementvaluenode c ON a.aud_id : c.aud_id WHERE a.aud_componentName LIKE 'tAggregate%' AND a.aud_typeField : 'OPERATIONS' AND b.aud_componentName LIKE 'tAggregate%' AND b.aud_typeField : 'OPERATIONS' AND c.aud_componentName LIKE 'tAggregate%' AND c.aud_typeField : 'OPERATIONS' AND a.aud_elementRef : 'INPUT_COLUMN' AND b.aud_elementRef : 'OUTPUT_COLUMN' AND c.aud_elementRef : 'FUNCTION'\", 'aud_elementnode': 'INSERT IGNORE INTO aud_elementnode (aud_componentName,aud_field,aud_nameElementNode,aud_show,aud_valueElementNode,aud_ComponementValue,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)', 'aud_contextjob': 'INSERT IGNORE INTO aud_contextjob (aud_environementContext,aud_nameContext,aud_prompt,aud_promptNeeded,aud_typeContext,aud_valueContext,aud_repositoryContextId,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?,?)', 'aud_contextgroupdetail': 'INSERT IGNORE INTO aud_contextgroupdetail (aud_nameContext,aud_commentContext,NameProject,NameContextGroup,exec_date) VALUES (?, ?, ?, ?, ?)', 'aud_node': 'INSERT IGNORE INTO aud_node (aud_componentName , aud_componentVersion , aud_offsetLabelX , aud_offsetLabelY , aud_posX , aud_posY , aud_componentValue , NameProject , NameJob , exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?,?)', 'aud_bigdata': 'INSERT IGNORE INTO aud_bigdata (aud_field,aud_name,aud_show,aud_value,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?)', 'aud_bigdata_elementvalue': 'INSERT IGNORE INTO aud_bigdata_elementvalue (aud_elementRef,aud_value,aud_name,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?)', 'aud_metadata': 'INSERT INTO aud_metadata(aud_connector,aud_labelConnector,aud_nameComponentView,aud_comment,aud_key,aud_length,aud_columnName,aud_nullable,aud_pattern,aud_precision,aud_sourceType,aud_type,aud_usefulColumn,aud_originalLength,aud_defaultValue,aud_componentValue,aud_componentName,NameProject,NameJob,exec_date) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?) ON DUPLICATE KEY UPDATE aud_connector = VALUES(aud_connector), aud_labelConnector = VALUES(aud_labelConnector), aud_nameComponentView = VALUES(aud_nameComponentView), aud_comment = VALUES(aud_comment), aud_key = VALUES(aud_key), aud_length = VALUES(aud_length), aud_columnName = VALUES(aud_columnName), aud_nullable = VALUES(aud_nullable), aud_pattern = VALUES(aud_pattern), aud_precision = VALUES(aud_precision), aud_sourceType = VALUES(aud_sourceType), aud_type = VALUES(aud_type), aud_usefulColumn = VALUES(aud_usefulColumn), aud_originalLength = VALUES(aud_originalLength), aud_defaultValue = VALUES(aud_defaultValue), aud_componentValue = VALUES(aud_componentValue), aud_componentName = VALUES(aud_componentName), exec_date = VALUES(exec_date)'}, 'Directories': {'items_directory': 'C:/Users/sonia/Downloads/KEOLISTOURS/KEOLISTOURS/process'}, 'database': {'type': 'mysql', 'postgresql': {'user': 'postgres', 'password': 'your_password', 'host': 'localhost', 'port': 5432, 'dbname': 'dataraise_test'}, 'mysql': {'user': 'root', 'password': 'Sa*201299', 'host': '127.0.0.1', 'port': 3306, 'dbname': 'sqops_dataraise'}, 'sqlite': {'filepath': 'path/to/your/sqlite.db'}}, 'ORA_ECH_ADMECH': {'CTX_ORA_ECH_ADMECH_Server': 'echdfr.cfo1dl4btaz7.eu-west-1.rds.amazonaws.com', 'CTX_ORA_ECH_ADMECH_Password': '', 'CTX_ORA_ECH_ADMECH_Login': 'ADM_ECH', 'CTX_ORA_ECH_ADMECH_Port': '1521', 'CTX_ORA_ECH_ADMECH_AdditionalParams': '', 'CTX_ORA_ECH_ADMECH_Schema': 'ADM_ECH', 'CTX_ORA_ECH_ADMECH_Sid': 'echdfr'}}\n",
      "JDBC Parameters: {'AUDIT_JDBC_connection_userPassword_password': 'Sa*201299', 'AUDIT_JDBC_drivers': 'C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar', 'AUDIT_JDBC_connection_driverClass': 'com.mysql.cj.jdbc.Driver', 'AUDIT_JDBC_mappingFile': 'mysql_id', 'AUDIT_JDBC_connection_userPassword_userId': 'root', 'AUDIT_JDBC_connection_jdbcUrl': 'jdbc:mysql://localhost:3306/sqops_dataraise?allowLoadLocalInfile=true'}\n",
      "JDBC Driver: com.mysql.cj.jdbc.Driver\n",
      "JDBC URL: jdbc:mysql://localhost:3306/sqops_dataraise?allowLoadLocalInfile=true\n",
      "JDBC User: root\n",
      "JDBC JAR: C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar\n",
      "parameter: items_directory -> C:/Users/sonia/Downloads/KEOLISTOURS/KEOLISTOURS/process\n",
      "parameter: TRANSVERSE_QUERY_LASTEXECUTIONDATE -> SELECT MAX(lastexecutiondate) as lastexecutiondate FROM executiondate\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "from config import Config  # Assuming Config class is defined in config.py\n",
    "from database import Database  # Assuming Database class is defined in database.py\n",
    "from XML_parse import XMLParser  # Importing the XMLParser class\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='database_operations.log',\n",
    "    level=logging.DEBUG,  # Changed to DEBUG to capture all messages\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w'  # Ensure the file is overwritten each time for clean logs\n",
    ")\n",
    "\n",
    "def write_to_csv(file_path: str, data: List[Tuple]):\n",
    "    \"\"\"Write the data to a CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(data)\n",
    "        logging.info(f\"Written {len(data)} rows to CSV file {file_path}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error writing to CSV file {file_path}: {e}\", exc_info=True)\n",
    "\n",
    "def AUD_304_ALIMMETADATA(config: Config, db: Database, parsed_files_data: List[Tuple[str, str, dict]]):\n",
    "    \"\"\"\n",
    "    Perform various database operations including retrieving JDBC parameters, \n",
    "    executing queries, deleting records, and inserting data.\n",
    "\n",
    "    Args:\n",
    "        config (Config): An instance of the Config class for retrieving configuration parameters.\n",
    "        db (Database): An instance of the Database class for executing database operations.\n",
    "        parsed_files_data (List[Tuple[str, str, dict]]): A list where each tuple contains (project_name, job_name, parsed_data).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Get the execution date\n",
    "        execution_date_query = config.get_param('queries', 'TRANSVERSE_QUERY_LASTEXECUTIONDATE')\n",
    "        execution_date = db.get_execution_date(execution_date_query)\n",
    "        logging.info(f\"Execution Date: {execution_date}\")\n",
    "\n",
    "        # Step 2: Execute LOCAL_TO_DBBRUT_QUERY\n",
    "        local_to_dbbrut_query = config.get_param('queries', 'LOCAL_TO_DBBRUT_QUERY')\n",
    "        logging.info(f\"Executing query: {local_to_dbbrut_query}\")  # Log the query before execution\n",
    "        local_to_dbbrut_query_results = db.execute_query(local_to_dbbrut_query)\n",
    "        logging.debug(f\"local_to_dbbrut_query_results: {local_to_dbbrut_query_results}\")\n",
    "\n",
    "        # Step 3: Delete the output from aud_node based on the query results\n",
    "        delete_conditions = []\n",
    "        for result in local_to_dbbrut_query_results:\n",
    "            project_name, job_name, _, _, _ = result  # Assuming result contains these fields in order\n",
    "            delete_conditions.append({\n",
    "                'NameProject': project_name,\n",
    "                'NameJob': job_name\n",
    "            })\n",
    "        db.delete_records_batch('aud_meta', delete_conditions)\n",
    "        logging.info(f\"Deleted records for projects/jobs: {[(d['NameProject'], d['NameJob']) for d in delete_conditions]}\")\n",
    "\n",
    "        # Step 4: Execute NOT_AUDITED_JOBS_QUERY\n",
    "        aud_metadata = config.get_param('queries', 'aud_metadata')\n",
    "        logging.info(f\"Executing query: {aud_metadata}\")  # Log the query before execution\n",
    "        aud_metadata_results = db.execute_query(aud_metadata)\n",
    "        logging.debug(f\"aud_metadata_results: {aud_metadata_results}\")\n",
    "\n",
    "        # Step 5: Delete the output from aud_elementvaluenode based on the query results\n",
    "        delete_conditions = []\n",
    "        for result in aud_metadata_results:\n",
    "            project_name, job_name = result\n",
    "            logging.debug(f\"Preparing to delete records for PROJECT_NAME: {project_name}, JOB_NAME: {job_name}\")\n",
    "            delete_conditions.append({\n",
    "                'NameProject': project_name,\n",
    "                'NameJob': job_name\n",
    "            })\n",
    "        db.delete_records_batch('aud_elementvaluenode', delete_conditions)\n",
    "        logging.info(f\"Deleted records for projects/jobs: {[(d['NameProject'], d['NameJob']) for d in delete_conditions]}\")\n",
    "\n",
    "        # Step 6: Prepare and write data to CSV\n",
    "        data_to_write = []\n",
    "        for project_name, job_name, parsed_data in parsed_files_data:\n",
    "            for node_data in parsed_data['nodes']:\n",
    "                for elem_param in node_data['elementParameters']:\n",
    "                    for elem_value in elem_param['elementValues']:\n",
    "                        for meta in node_data['metadata']:\n",
    "                            for column in meta['columns']:\n",
    "                                params = (\n",
    "                                    meta['connector'],\n",
    "                                    meta['label'],\n",
    "                                    meta['name'],\n",
    "                                    column['comment'],\n",
    "                                    int(column['key'] != 'false'),\n",
    "                                    column['length'],\n",
    "                                    column['name'],\n",
    "                                    int(column['nullable'] != 'false'),\n",
    "                                    column['pattern'],\n",
    "                                    column['precision'],\n",
    "                                    column['sourceType'],\n",
    "                                    column['type'],\n",
    "                                    int(column['usefulColumn'] != 'false'),\n",
    "                                    column['originalLength'],\n",
    "                                    column['defaultValue'],\n",
    "                                    elem_value['value'],\n",
    "                                    node_data['componentName'],\n",
    "                                    project_name,\n",
    "                                    job_name,\n",
    "                                    execution_date\n",
    "                                )\n",
    "                                data_to_write.append(params)\n",
    "\n",
    "        csv_file_path = \"output.csv\"  # Example path; adjust as needed\n",
    "        write_to_csv(csv_file_path, data_to_write)\n",
    "\n",
    "        # Step 8: Bulk insert data from the CSV file into the `aud_metadata` table\n",
    "        db.insert_data_row_by_row('aud_metadata', csv_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if db:\n",
    "            db.close()  # Ensure the database connection is closed\n",
    "            logging.info(\"Database connection closed\")\n",
    "\n",
    "def main():\n",
    "    config_file = \"configs/config.yaml\"\n",
    "    config = Config(config_file)\n",
    "\n",
    "    jdbc_params = config.get_jdbc_parameters()\n",
    "    logging.info(f\"JDBC Parameters: {jdbc_params}\")\n",
    "\n",
    "    db = Database(jdbc_params)\n",
    "    db.set_jdbc_parameters(jdbc_params)  # Set JDBC parameters if needed\n",
    "    db.connect_JDBC()  # Test the JDBC connection\n",
    "\n",
    "    items_directory = config.get_param('Directories', 'items_directory')\n",
    "    xml_parser = XMLParser()\n",
    "    parsed_files_data = xml_parser.loop_parse(items_directory)\n",
    "    logging.info(f\"Parsed files data: {parsed_files_data}\")\n",
    "\n",
    "    AUD_304_ALIMMETADATA(config, db, parsed_files_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration: {'Audit_JDBC': {'AUDIT_JDBC_connection_userPassword_password': 'Sa*201299', 'AUDIT_JDBC_drivers': 'C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar', 'AUDIT_JDBC_connection_driverClass': 'com.mysql.cj.jdbc.Driver', 'AUDIT_JDBC_mappingFile': 'mysql_id', 'AUDIT_JDBC_connection_userPassword_userId': 'root', 'AUDIT_JDBC_connection_jdbcUrl': 'jdbc:mysql://localhost:3306/sqops_dataraise?allowLoadLocalInfile=true'}, 'queries': {'LOCAL_TO_DBBRUT_QUERY': 'select distinct PROJECT_NAME, JOB_NAME, JOB_PATH, JOB_VERSION, Talend_Version from audit_jobs_delta where talend_version is not null and niveau is not null', 'LOCAL_TO_DBBRUT_FILTER_name_elementNode': \"where aud_nameElementNode in ('DBNAME','TYPE','QUERY','TABLE','FILENAME','TEMPDIR','sql','query')\", 'LOCAL_TO_DBBRUT_FILTER_columnName_metadata': \"where aud_columnName not in ('errorCode','errorMessage')\", 'LOCAL_TO_DBBRUT_QUERY_ROUTINES': 'select distinct PROJECT_NAME, ROUTINE_NAME, ROUTINE_PATH, ROUTINE_VERSION, Talend_Version from audit_routines where talend_version is not null', 'LOCAL_TO_DBBRUT_QUERY_CONTEXTGROUP': 'select distinct PROJECT_NAME, CONTEXT_NAME, CONTEXT_PATH, CONTEXT_VERSION, Talend_Version from audit_contextgroup where talend_version is not null', 'LOCAL_TO_DBBRUT_QUERY_METADATA': 'select distinct PROJECT_NAME, METADATA_NAME, METADATA_PATH, METADATA_VERSION, Talend_Version from audit_metadata where talend_version is not null', 'TRANSVERSE_QUERY_LASTEXECUTIONDATE': 'SELECT MAX(lastexecutiondate) as lastexecutiondate FROM executiondate', 'LOCAL_TO_DBBRUT_QUERY_JOBLETS': 'select distinct PROJECT_NAME, JOBLET_NAME, JOBLET_PATH, JOBLET_VERSION, Talend_Version from audit_joblets where talend_version is not null', 'aud_elementnode': 'select distinct namejob, nameproject from aud_elementnode where NameJob not in (select job_name from audit_jobs)', 'aud_contextjob': 'select distinct namejob, nameproject from aud_contextjob where NameJob not in (select job_name from audit_jobs)', 'aud_node': 'select distinct namejob, nameproject from aud_node where NameJob not in (select job_name from audit_jobs)', 'NodeJoinElementnode': \"select distinct N.NameProject, N.NameJob, N.aud_componentValue from aud_node N inner join aud_elementnode E on (N.namejob = E.namejob and N.nameproject = E.nameproject and N.aud_componentValue = E.aud_ComponementValue) where E.aud_nameElementNode = 'ACTIVATE' and E.aud_valueElementNode = 'false' and E.exec_date = (select max(lastexecutiondate) from executiondate)\", 'aud_bigdata': 'select distinct namejob, nameproject from aud_bigdata where NameJob not in (select job_name from audit_jobs)', 'aud_metadata': 'select distinct namejob, nameproject from aud_metadata where NameJob not in (select job_name from audit_jobs)', 'MetadataJoinElemntnode': \"select  distinct N.NameProject, N.NameJob, N.aud_componentValue from aud_metadata N inner join aud_elementnode E on (N.namejob = E.namejob and N.nameproject = E.nameproject and N.aud_componentValue = E.aud_ComponementValue) where E.aud_nameElementNode = 'ACTIVATE' and E.aud_valueElementNode = 'false' and E.exec_date = (select max(lastexecutiondate) from executiondate)\"}, 'insert_queries': {'aud_agg_aggregate': \"INSERT IGNORE INTO aud_agg_aggregate (NameProject, namejob, aud_componentValue, aud_valueElementRef_input, aud_valueElementRef_output, aud_valueElementRef_function) SELECT a.NameProject, a.namejob, a.aud_componentValue, a.aud_valueElementRef as aud_valueElementRef_input, b.aud_valueElementRef as aud_valueElementRef_output, 'GROUPBY' as aud_valueElementRef_function FROM aud_elementvaluenode a INNER JOIN aud_elementvaluenode b ON a.aud_id : b.aud_id WHERE a.aud_typeField IN ('GROUPBYS') AND b.aud_typeField IN ('GROUPBYS') AND a.aud_elementRef : 'INPUT_COLUMN' AND b.aud_elementRef : 'OUTPUT_COLUMN' AND a.aud_componentName LIKE 'tAggregate%' AND b.aud_componentName LIKE 'tAggregate%' UNION ALL SELECT a.NameProject, a.namejob, a.aud_componentValue, a.aud_valueElementRef as aud_valueElementRef_input, b.aud_valueElementRef as aud_valueElementRef_output , c.aud_valueElementRef as aud_valueElementRef_function FROM aud_elementvaluenode a INNER JOIN aud_elementvaluenode b ON a.aud_id : b.aud_id INNER JOIN aud_elementvaluenode c ON a.aud_id : c.aud_id WHERE a.aud_componentName LIKE 'tAggregate%' AND a.aud_typeField : 'OPERATIONS' AND b.aud_componentName LIKE 'tAggregate%' AND b.aud_typeField : 'OPERATIONS' AND c.aud_componentName LIKE 'tAggregate%' AND c.aud_typeField : 'OPERATIONS' AND a.aud_elementRef : 'INPUT_COLUMN' AND b.aud_elementRef : 'OUTPUT_COLUMN' AND c.aud_elementRef : 'FUNCTION'\", 'aud_elementnode': 'INSERT IGNORE INTO aud_elementnode (aud_componentName,aud_field,aud_nameElementNode,aud_show,aud_valueElementNode,aud_ComponementValue,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)', 'aud_contextjob': 'INSERT IGNORE INTO aud_contextjob (aud_environementContext,aud_nameContext,aud_prompt,aud_promptNeeded,aud_typeContext,aud_valueContext,aud_repositoryContextId,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?,?)', 'aud_contextgroupdetail': 'INSERT IGNORE INTO aud_contextgroupdetail (aud_nameContext,aud_commentContext,NameProject,NameContextGroup,exec_date) VALUES (?, ?, ?, ?, ?)', 'aud_node': 'INSERT IGNORE INTO aud_node (aud_componentName , aud_componentVersion , aud_offsetLabelX , aud_offsetLabelY , aud_posX , aud_posY , aud_componentValue , NameProject , NameJob , exec_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?,?)', 'aud_bigdata': 'INSERT IGNORE INTO aud_bigdata (aud_field,aud_name,aud_show,aud_value,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?, ?)', 'aud_bigdata_elementvalue': 'INSERT IGNORE INTO aud_bigdata_elementvalue (aud_elementRef,aud_value,aud_name,NameProject,NameJob,exec_date) VALUES (?, ?, ?, ?, ?, ?)', 'aud_metadata': 'INSERT INTO aud_metadata(aud_connector,aud_labelConnector,aud_nameComponentView,aud_comment,aud_key,aud_length,aud_columnName,aud_nullable,aud_pattern,aud_precision,aud_sourceType,aud_type,aud_usefulColumn,aud_originalLength,aud_defaultValue,aud_componentValue,aud_componentName,NameProject,NameJob,exec_date) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?) ON DUPLICATE KEY UPDATE aud_connector = VALUES(aud_connector), aud_labelConnector = VALUES(aud_labelConnector), aud_nameComponentView = VALUES(aud_nameComponentView), aud_comment = VALUES(aud_comment), aud_key = VALUES(aud_key), aud_length = VALUES(aud_length), aud_columnName = VALUES(aud_columnName), aud_nullable = VALUES(aud_nullable), aud_pattern = VALUES(aud_pattern), aud_precision = VALUES(aud_precision), aud_sourceType = VALUES(aud_sourceType), aud_type = VALUES(aud_type), aud_usefulColumn = VALUES(aud_usefulColumn), aud_originalLength = VALUES(aud_originalLength), aud_defaultValue = VALUES(aud_defaultValue), aud_componentValue = VALUES(aud_componentValue), aud_componentName = VALUES(aud_componentName), exec_date = VALUES(exec_date)'}, 'Directories': {'items_directory': 'C:/Users/sonia/Downloads/KEOLISTOURS/KEOLISTOURS/process'}, 'database': {'type': 'mysql', 'postgresql': {'user': 'postgres', 'password': 'your_password', 'host': 'localhost', 'port': 5432, 'dbname': 'dataraise_test'}, 'mysql': {'user': 'root', 'password': 'Sa*201299', 'host': '127.0.0.1', 'port': 3306, 'dbname': 'sqops_dataraise'}, 'sqlite': {'filepath': 'path/to/your/sqlite.db'}}, 'ORA_ECH_ADMECH': {'CTX_ORA_ECH_ADMECH_Server': 'echdfr.cfo1dl4btaz7.eu-west-1.rds.amazonaws.com', 'CTX_ORA_ECH_ADMECH_Password': '', 'CTX_ORA_ECH_ADMECH_Login': 'ADM_ECH', 'CTX_ORA_ECH_ADMECH_Port': '1521', 'CTX_ORA_ECH_ADMECH_AdditionalParams': '', 'CTX_ORA_ECH_ADMECH_Schema': 'ADM_ECH', 'CTX_ORA_ECH_ADMECH_Sid': 'echdfr'}}\n",
      "JDBC Parameters: {'AUDIT_JDBC_connection_userPassword_password': 'Sa*201299', 'AUDIT_JDBC_drivers': 'C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar', 'AUDIT_JDBC_connection_driverClass': 'com.mysql.cj.jdbc.Driver', 'AUDIT_JDBC_mappingFile': 'mysql_id', 'AUDIT_JDBC_connection_userPassword_userId': 'root', 'AUDIT_JDBC_connection_jdbcUrl': 'jdbc:mysql://localhost:3306/sqops_dataraise?allowLoadLocalInfile=true'}\n",
      "JDBC Driver: com.mysql.cj.jdbc.Driver\n",
      "JDBC URL: jdbc:mysql://localhost:3306/sqops_dataraise?allowLoadLocalInfile=true\n",
      "JDBC User: root\n",
      "JDBC JAR: C:/Users/sonia/Downloads/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0/mysql-connector-j-9.0.0.jar\n",
      "parameter: items_directory -> C:/Users/sonia/Downloads/KEOLISTOURS/KEOLISTOURS/process\n",
      "parameter: TRANSVERSE_QUERY_LASTEXECUTIONDATE -> SELECT MAX(lastexecutiondate) as lastexecutiondate FROM executiondate\n",
      "parameter: LOCAL_TO_DBBRUT_QUERY -> select distinct PROJECT_NAME, JOB_NAME, JOB_PATH, JOB_VERSION, Talend_Version from audit_jobs_delta where talend_version is not null and niveau is not null\n",
      "parameter: aud_metadata -> select distinct namejob, nameproject from aud_metadata where NameJob not in (select job_name from audit_jobs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\jaydebeapi\\__init__.py:188: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  ver_match = re.match('\\d+\\.\\d+', jpype.__version__)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 139\u001b[0m\n\u001b[0;32m    136\u001b[0m     AUD_304_ALIMMETADATA(config, db, parsed_files_data)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 136\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m parsed_files_data \u001b[38;5;241m=\u001b[39m xml_parser\u001b[38;5;241m.\u001b[39mloop_parse(items_directory)\n\u001b[0;32m    134\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsed files data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_files_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 136\u001b[0m \u001b[43mAUD_304_ALIMMETADATA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed_files_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 106\u001b[0m, in \u001b[0;36mAUD_304_ALIMMETADATA\u001b[1;34m(config, db, parsed_files_data, batch_size)\u001b[0m\n\u001b[0;32m    104\u001b[0m                         \u001b[38;5;66;03m# If the batch size is reached, insert the data\u001b[39;00m\n\u001b[0;32m    105\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_batch) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[1;32m--> 106\u001b[0m                             \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maud_metadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m                             data_batch\u001b[38;5;241m.\u001b[39mclear()  \u001b[38;5;66;03m# Clear the batch after insertion\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Insert any remaining data that didn't fill a complete batch\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# if data_batch:\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# db.insert_metadata('aud_metadata', data_batch)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sonia\\Desktop\\SQOPS_SERVER_PY\\SQOPS_SERVER_PY\\database.py:119\u001b[0m, in \u001b[0;36mDatabase.insert_metadata\u001b[1;34m(self, table_name, data_batch)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data_batch:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m         \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    121\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping row due to error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, row data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\jaydebeapi\\__init__.py:531\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, operation, parameters)\u001b[0m\n\u001b[0;32m    529\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_last()\n\u001b[1;32m--> 531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepareStatement\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_stmt_parms(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep, parameters)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "from config import Config  # Assuming Config class is defined in config.py\n",
    "from database import Database  # Assuming Database class is defined in database.py\n",
    "from XML_parse import XMLParser  # Importing the XMLParser class\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='database_operations.log',\n",
    "    level=logging.DEBUG,  # Set to DEBUG to capture all messages\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w'  # Overwrite the log file each time for clean logs\n",
    ")\n",
    "\n",
    "def AUD_304_ALIMMETADATA(config: Config, db: Database, parsed_files_data: List[Tuple[str, str, dict]], batch_size=100):\n",
    "    \"\"\"\n",
    "    Perform various database operations including retrieving JDBC parameters, \n",
    "    executing queries, deleting records, and inserting data.\n",
    "\n",
    "    Args:\n",
    "        config (Config): An instance of the Config class for retrieving configuration parameters.\n",
    "        db (Database): An instance of the Database class for executing database operations.\n",
    "        parsed_files_data (List[Tuple[str, str, dict]]): A list where each tuple contains (project_name, job_name, parsed_data).\n",
    "        batch_size (int): The number of rows to insert in each batch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Get the execution date\n",
    "        execution_date_query = config.get_param('queries', 'TRANSVERSE_QUERY_LASTEXECUTIONDATE')\n",
    "        execution_date = db.get_execution_date(execution_date_query)\n",
    "        logging.info(f\"Execution Date: {execution_date}\")\n",
    "\n",
    "        # Step 2: Execute LOCAL_TO_DBBRUT_QUERY\n",
    "        local_to_dbbrut_query = config.get_param('queries', 'LOCAL_TO_DBBRUT_QUERY')\n",
    "        logging.info(f\"Executing query: {local_to_dbbrut_query}\")\n",
    "        local_to_dbbrut_query_results = db.execute_query(local_to_dbbrut_query)\n",
    "        logging.debug(f\"local_to_dbbrut_query_results: {local_to_dbbrut_query_results}\")\n",
    "\n",
    "        # Step 3: Delete the output from aud_metadata based on the query results\n",
    "        delete_conditions = []\n",
    "        for result in local_to_dbbrut_query_results:\n",
    "            project_name, job_name, _, _, _ = result  # Assuming result contains these fields in order\n",
    "            delete_conditions.append({\n",
    "                'NameProject': project_name,\n",
    "                'NameJob': job_name\n",
    "            })\n",
    "        db.delete_records_batch('aud_metadata', delete_conditions)\n",
    "        logging.info(f\"Deleted records for projects/jobs: {[(d['NameProject'], d['NameJob']) for d in delete_conditions]}\")\n",
    "\n",
    "        # Step 4: Execute aud_metadata query\n",
    "        aud_metadata_query = config.get_param('queries', 'aud_metadata')\n",
    "        logging.info(f\"Executing query: {aud_metadata_query}\")\n",
    "        aud_metadata_results = db.execute_query(aud_metadata_query)\n",
    "        logging.debug(f\"aud_metadata_results: {aud_metadata_results}\")\n",
    "\n",
    "        # Step 5: Delete records based on the aud_metadata query results\n",
    "        delete_conditions = []\n",
    "        for result in aud_metadata_results:\n",
    "            project_name, job_name = result\n",
    "            logging.debug(f\"Preparing to delete records for PROJECT_NAME: {project_name}, JOB_NAME: {job_name}\")\n",
    "            delete_conditions.append({\n",
    "                'NameProject': project_name,\n",
    "                'NameJob': job_name\n",
    "            })\n",
    "        db.delete_records_batch('aud_metadata', delete_conditions)\n",
    "        logging.info(f\"Deleted records for projects/jobs: {[(d['NameProject'], d['NameJob']) for d in delete_conditions]}\")\n",
    "\n",
    "        # Step 6: Collect parsed parameters data into batches\n",
    "        data_batch = []\n",
    "        for project_name, job_name, parsed_data in parsed_files_data:\n",
    "            for node_data in parsed_data['nodes']:\n",
    "                for elem_param in node_data['elementParameters']:\n",
    "                    for elem_value in elem_param['elementValues']:\n",
    "                        for meta in node_data['metadata']:\n",
    "                            for column in meta['columns']:\n",
    "                                params = (\n",
    "                                    meta['connector'],\n",
    "                                    meta['label'],\n",
    "                                    meta['name'],\n",
    "                                    column['comment'],\n",
    "                                    int(column['key'] != 'false'),\n",
    "                                    column['length'],\n",
    "                                    column['name'],\n",
    "                                    int(column['nullable'] != 'false'),\n",
    "                                    column['pattern'],\n",
    "                                    column['precision'],\n",
    "                                    column['sourceType'],\n",
    "                                    column['type'],\n",
    "                                    int(column['usefulColumn'] != 'false'),\n",
    "                                    column['originalLength'],\n",
    "                                    column['defaultValue'],\n",
    "                                    elem_value['value'],\n",
    "                                    node_data['componentName'],\n",
    "                                    project_name,\n",
    "                                    job_name,\n",
    "                                    execution_date\n",
    "                                )\n",
    "                                data_batch.append(params)\n",
    "\n",
    "                                # If the batch size is reached, insert the data\n",
    "                                if len(data_batch) == batch_size:\n",
    "                                    db.insert_metadata('aud_metadata', data_batch)\n",
    "                                    data_batch.clear()  # Clear the batch after insertion\n",
    "\n",
    "        # Insert any remaining data that didn't fill a complete batch\n",
    "        if data_batch:\n",
    "            db.insert_metadata('aud_metadata', data_batch)\n",
    "\n",
    "        # Step 7: Execute MetadataJoinElemntnode query\n",
    "        metadata_join_element_node_query = config.get_param('queries', 'MetadataJoinElemntnode')\n",
    "        logging.info(f\"Executing query: {metadata_join_element_node_query}\")\n",
    "        metadata_join_element_node_results = db.execute_query(metadata_join_element_node_query)\n",
    "        logging.debug(f\"MetadataJoinElemntnode_results: {metadata_join_element_node_results}\")\n",
    "\n",
    "        # Step 8: Delete records based on MetadataJoinElemntnode query results\n",
    "        delete_conditions = []\n",
    "        for result in metadata_join_element_node_results:\n",
    "            project_name, job_name = result\n",
    "            logging.debug(f\"Preparing to delete records for PROJECT_NAME: {project_name}, JOB_NAME: {job_name}\")\n",
    "            delete_conditions.append({\n",
    "                'NameProject': project_name,\n",
    "                'NameJob': job_name\n",
    "            })\n",
    "        db.delete_records_batch('aud_metadata', delete_conditions)\n",
    "        logging.info(f\"Deleted records for projects/jobs: {[(d['NameProject'], d['NameJob']) for d in delete_conditions]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if db:\n",
    "            db.close()  # Ensure the database connection is closed\n",
    "            logging.info(\"Database connection closed\")\n",
    "\n",
    "def main():\n",
    "    config_file = \"configs/config.yaml\"\n",
    "    config = Config(config_file)\n",
    "\n",
    "    jdbc_params = config.get_jdbc_parameters()\n",
    "    logging.info(f\"JDBC Parameters: {jdbc_params}\")\n",
    "\n",
    "    db = Database(jdbc_params)\n",
    "    db.set_jdbc_parameters(jdbc_params)  # Set JDBC parameters if needed\n",
    "    db.connect_JDBC()  # Test the JDBC connection\n",
    "\n",
    "    items_directory = config.get_param('Directories', 'items_directory')\n",
    "    xml_parser = XMLParser()\n",
    "    parsed_files_data = xml_parser.loop_parse(items_directory)\n",
    "    logging.info(f\"Parsed files data: {parsed_files_data}\")\n",
    "\n",
    "    AUD_304_ALIMMETADATA(config, db, parsed_files_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
